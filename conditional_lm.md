# Conditional Language Model

20181117

一般的语言模型是预测一个序列的概率，而条件语言模型是在给定一个条件的前提下，预测一个序列的概率，经常是在给定一个序列的前提下，预测另一个与之相关的序列的概率。

某些条件语言模型可以用CRF来解决，对于CRF，在预测的时候可以使用维特比算法，得到全局最优序列；但是，对于用RNN建立的条件语言模型，在预测的时候，无法得到全局最优解，通常是用greedy search, beam search或者随机搜索来得到次优解。

CRF能解决的语言模型要求预测的序列与给定的序列有一一对应的关系，这是很强的要求，导致很多问题没法放在这个框架内解决。另一方面，CRF相比更先进的语言模型，还是有局限，第一，给定的序列虽然整体上都能影响预测的序列，但影响的方式只能是CRF的方式，而深度学习就有多种影响方式，比如，可以是给定序列的句向量影响每一个预测序列字符的输出，也可以是lstm的hidden state影响每一个预测序列字符的输出；第二，CRF一般是linear chain CRF，意味着预测序列字符只受上一个预测序列字符的影响，而深度学习可以把之前所有的预测序列字符的信息压缩在一个向量或矩阵中，影响下一个预测序列的字符，从而摆脱了n-gram model的假设（虽然在实际上，long range dependency还是很难抓住，效果与n-gram model是类似的）

在评估条件语言模型时，理论上存在三种办法，一种是cross entropy，可以实现，但是很难理解，一种是task specific evaluation，比如bleu, rouge，meteor，wer，容易实现，可以理解，第三种是人工评估，实现成本巨高，但最容易理解。一般使用第二种方法。

在建立条件语言模型时，主要面临两个问题：

第一，How do we encode condition x as a fixed-size vector, c? It is problem (or at least modality) specific.（之前是这种认识，随着多任务拟合的出现，这种认识正在发生改变）

第二，How do we condition on c in the decoding model? It is less problem specific.

一种方法是计算句向量用average pooling或者cnn，cnn的好处是能够学到local context，多个相邻的local context还可能得到长距离依赖，另外，cnn有树一样的分支结构，但不需要句法分析，cnn的速度也比rnn要快。

有人说average pooling比concatenate效果要好，但也有人反着说，所以，还是要针对具体任务做评估。

问题在于，cnn一般是用于固定大小的图片的，对于句子，也要求固定大小的句子，对于短的句子可以用0补上，但是，对于长的句子，就要截断，丢失信息。用rnn理论上可以处理任意长度的句子，但是，在训练时，对于mini-batch的一批训练，RNN的长度是固定的，正因为如此，所以用到bucket，先做长度聚类。对于最新的tensorflow 不需要使用 bucketing了，直接用 dynamic rnn 就好，它会根据每个batch自动计算最好的输出(其实就是这个batch中最长的序列，还是用bucketing节省计算资源)，不过要更定每个example的 sequence length。当然，现在有人可以做到自动计算 sequence length了（https://www.zhihu.com/question/42057513/answer/128885542）。

decoder用的是普通rnn，只有hidden state，没有cell state，但条件的句向量每次都和矩阵变换后的hidden state并列，然后进入到激活函数中，用来预测下一个词是什么

另一种方法是常用的seq2seq，是用两个lstm，同时，输入的序列可以是正序，也可以是倒序，有时候倒序效果要更好。这个模型的问题是，encoder的output作为decoder的输入，要记住之前的太多的信息。

使用多个seq2seq模型的集成，效果有可能更好。

事实上，文本分类问题也可以放在语言模型的框架下来解决，encoder是类似的，decoder非常简单，只要输出一个字符就行，如果是固定的类别，最为简单，如果是可变的有限字符组成的集合中的类别，也能用pointer net来处理。

机器学习模型有两种简单的分类方法：

一种是，自然语言处理，图像视频处理等基于应用场景的。

另一种是，独立变量模型、时间序列模型等基于数据特点的，自然语言处理是时间序列模型的一种，拿序列标记来说，如果不是时间序列的话，就是一个分类问题，但因为是事件序列的分类，模型要更加复杂。同理，聚类也是，时间序列的聚类要比独立变量的聚类更加复杂。一般的机器学习研究的都是独立变量模型，但时间序列模型在应用上更加管用，一般的机器学习只是个基础，很少能直接拿来用。

如果词向量用长度为2的向量表示（二维空间），句向量也用长度为2的向量表示的话，句向量就是二维空间中的一个点，求句子相似度可以用cosine similarity，也可以用euclidean distance。如果句向量作为词向量的时间序列的话，句向量就是二维空间中的一条曲线（把点按时间先后连起来），求句子相似度就是看空间中的两条曲线的相似度。或者是三维空间中的一条曲线，第三个维度就是时间先后。

早期的NNLM模型，也是语言模型，就是知道上文预测下文，从而预测整个句子的概率，每次用之前两个词的词向量连起来放入神经网络预测下一个词，本质上是n-gram language model。神经网络模型都是参数方法，参数是拟合的，而某些HMM，是非参数方法，参数是直接由数据通过解析式计算的。对于自然语言生成型任务，都是知道上文预测下文（比如输入法提示）。但也有些任务，是知道上下文预测中间某个词，比如作文修改、语法订正等，还包括word2vec等词向量拟合方法。知道上下文预测中间某个词，一般是用CBOW（continuous bag of words），用层次softmax（加快参数拟合速度），把词向量连起来作为输入的向量。skip-gram是中间的词预测周围的词，有几种处理办法，一是周围的词的距离不作区分，最简单，实际上也是用的这种办法，用中间词和周围词的词向量就能算出周围词是各个词的概率；二是周围的词按不同距离各自使用一个数学模型，比如距离为1的词是一个模型，距离为2的词是另一个独立的模型；三是周围的词使用同一个数学模型，但不是简单的等同处理，而是使用一个时间序列模型。

词本来是one hot encoding，通过矩阵变换就变成了word embedding，得到embedding之后(此处是word embedding)的处理，可以是concatenate，然后进一步做矩阵变换，或者得到的embedding就简单相加，得到句子的embedding.

20181124

word2vec是固定的词向量，不能解决多义词的问题。而ELMo (Embedding from Language Model)是考虑上下文的动态的词向量，可以在一定程度上解决多义词的问题。一般用双层双向lstm来做，任务可以是一般的分类任务，或者是像word2vec一样给句子的每个位置的词分类。最初的输入还是固定的词向量，第一层双向lstm，可以认为从上下文中抽取了句法信息，第二层双向lstm，可以认为从上下文中抽取了语义信息（但这种说法是不严谨的，并没有很强的证据来证明这样的说法）。不管怎么样，通过双层双向lstm，得到了包含上下文信息的word embedding.

word2vec在做语言模型任务时，比如从中间的词预测周围的词（不是一般意义上的语言模型，因为语言模型预测词时很少只根据一个词来预测，除非是1-gram语言模型，一个词的预测只与上一个词是什么有关），用到中间的词的向量和周围词的向量，因为周围词是未知的，所以理论上所有词都要考虑，但因为是cross entropy，所以分子就只考虑文本中出现的词的词向量，而由于使用了negative sampling策略，分母中也只考虑文本中没出现过的少量词的词向量。

双层双向lstm得到了上下文的信息，但效果未必有简单的CBOW好，简单把上下文向量求平均也能得到关于上下文的信息。

word2vec和ELMo的拟合用的都是语言模型，也就是预测单词是什么，而且都使用的上下文的信息（有些语言模型只用上文的信息，不用下文的信息，比如输入法给提示的时候就用这样的语言模型）

GPT全称是generative(不是general?) pre-training，特征抽取用的是transformer，但只用了上文的信息，没用下文的信息。GPT还有个特点是多任务拟合，把多个任务使用相同的网络结构来拟合。这样，在下游任务中，transfer learing就不只是词向量了，而是整个网络。在多任务拟合上，salesforce的decanlp想法是类似的，只是网络结构不同。需要分析GPT、BERT、decanlp各自的网络结构，尤其是词向量动态表征之后的网络结构。

nlp的四大任务：序列标注，分类，句子关系判断（比如相似度判断，比如多轮对话中两句问话是否同一意图的关系判断，不如两句话是不是连着的两句话），生成式任务。

bert提出的masked语言模型是预测词的，同时还有预测句子的任务next sequence prediction，所以是多任务模型。预测词不是像word2vec一样每个词都预测，而是随机抠掉15%的词，用周围词的词向量来预测。

预测句子至少有两种办法，一种是语言模型，根据词的条件概率算出句子的概率，另一种是把句子当做整体，先算出句子的向量，然后像预测词一样利用句向量来预测句子。

在bert当中，每个单词有三个embedding，一是位置的embedding，二是单词的动态embedding，三是所在句子的embedding.但句子的embedding在bert当中怎么算，还需要具体看论文。

两阶段模型就是先预训练词向量或者整个网络，之后两种方案，一种是冻住一部分网络，放开一部分网络（初始值取之前训练好的值），重新训练，还有一种是所有网络全部放开，但初始值取之前训练好的值。

