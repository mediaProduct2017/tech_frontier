# Conditional Language Model

20181117

一般的语言模型是预测一个序列的概率，而条件语言模型是在给定一个条件的前提下，预测一个序列的概率，经常是在给定一个序列的前提下，预测另一个与之相关的序列的概率。

某些条件语言模型可以用CRF来解决，对于CRF，在预测的时候可以使用维特比算法，得到全局最优序列；但是，对于用RNN建立的条件语言模型，在预测的时候，无法得到全局最优解，通常是用greedy search, beam search或者随机搜索来得到次优解。

CRF能解决的语言模型要求预测的序列与给定的序列有一一对应的关系，这是很强的要求，导致很多问题没法放在这个框架内解决。另一方面，CRF相比更先进的语言模型，还是有局限，第一，给定的序列虽然整体上都能影响预测的序列，但影响的方式只能是CRF的方式，而深度学习就有多种影响方式，比如，可以是给定序列的句向量影响每一个预测序列字符的输出，也可以是lstm的hidden state影响每一个预测序列字符的输出；第二，CRF一般是linear chain CRF，意味着预测序列字符只受上一个预测序列字符的影响，而深度学习可以把之前所有的预测序列字符的信息压缩在一个向量或矩阵中，影响下一个预测序列的字符，从而摆脱了n-gram model的假设（虽然在实际上，long range dependency还是很难抓住，效果与n-gram model是类似的）

在评估条件语言模型时，理论上存在三种办法，一种是cross entropy，可以实现，但是很难理解，一种是task specific evaluation，比如bleu, rouge，meteor，wer，容易实现，可以理解，第三种是人工评估，实现成本巨高，但最容易理解。一般使用第二种方法。

在建立条件语言模型时，主要面临两个问题：

第一，How do we encode condition x as a fixed-size vector, c? It is problem (or at least modality) specific.（之前是这种认识，随着多任务拟合的出现，这种认识正在发生改变）

第二，How do we condition on c in the decoding model? It is less problem specific.

一种方法是计算句向量用average pooling或者cnn，cnn的好处是能够学到local context，多个相邻的local context还可能得到长距离依赖，另外，cnn有树一样的分支结构，但不需要句法分析，cnn的速度也比rnn要快。

有人说average pooling比concatenate效果要好，但也有人反着说，所以，还是要针对具体任务做评估。

问题在于，cnn一般是用于固定大小的图片的，对于句子，也要求固定大小的句子，对于短的句子可以用0补上，但是，对于长的句子，就要截断，丢失信息。用rnn理论上可以处理任意长度的句子，但是，在训练时，对于mini-batch的一批训练，RNN的长度是固定的，正因为如此，所以用到bucket，先做长度聚类。对于最新的tensorflow 不需要使用 bucketing了，直接用 dynamic rnn 就好，它会根据每个batch自动计算最好的输出(其实就是这个batch中最长的序列，还是用bucketing节省计算资源)，不过要更定每个example的 sequence length。当然，现在有人可以做到自动计算 sequence length了（https://www.zhihu.com/question/42057513/answer/128885542）。

decoder用的是普通rnn，只有hidden state，没有cell state，但条件的句向量每次都和矩阵变换后的hidden state并列，然后进入到激活函数中，用来预测下一个词是什么

另一种方法是常用的seq2seq，是用两个lstm，同时，输入的序列可以是正序，也可以是倒序，有时候倒序效果要更好。这个模型的问题是，encoder的output作为decoder的输入，要记住之前的太多的信息。

使用多个seq2seq模型的集成，效果有可能更好。

事实上，文本分类问题也可以放在语言模型的框架下来解决，encoder是类似的，decoder非常简单，只要输出一个字符就行，如果是固定的类别，最为简单，如果是可变的有限字符组成的集合中的类别，也能用pointer net来处理。

机器学习模型有两种简单的分类方法：

一种是，自然语言处理，图像视频处理等基于应用场景的。

另一种是，独立变量模型、时间序列模型等基于数据特点的，自然语言处理是时间序列模型的一种，拿序列标记来说，如果不是时间序列的话，就是一个分类问题，但因为是事件序列的分类，模型要更加复杂。同理，聚类也是，时间序列的聚类要比独立变量的聚类更加复杂。一般的机器学习研究的都是独立变量模型，但时间序列模型在应用上更加管用，一般的机器学习只是个基础，很少能直接拿来用。

如果词向量用长度为2的向量表示（二维空间），句向量也用长度为2的向量表示的话，句向量就是二维空间中的一个点，求句子相似度可以用cosine similarity，也可以用euclidean distance。如果句向量作为词向量的时间序列的话，句向量就是二维空间中的一条曲线（把点按时间先后连起来），求句子相似度就是看空间中的两条曲线的相似度。或者是三维空间中的一条曲线，第三个维度就是时间先后。

早期的NNLM模型，也是语言模型，就是知道上文预测下文，从而预测整个句子的概率，每次用之前两个词的词向量连起来放入神经网络预测下一个词，本质上是n-gram language model。神经网络模型都是参数方法，参数是拟合的，而某些HMM，是非参数方法，参数是直接由数据通过解析式计算的。对于自然语言生成型任务，都是知道上文预测下文（比如输入法提示）。但也有些任务，是知道上下文预测中间某个词，比如作文修改、语法订正等，还包括word2vec等词向量拟合方法。知道上下文预测中间某个词，一般是用CBOW（continuous bag of words），用层次softmax（加快参数拟合速度），把词向量连起来作为输入的向量。skip-gram是中间的词预测周围的词，有几种处理办法，一是周围的词的距离不作区分，最简单，实际上也是用的这种办法，用中间词和周围词的词向量就能算出周围词是各个词的概率；二是周围的词按不同距离各自使用一个数学模型，比如距离为1的词是一个模型，距离为2的词是另一个独立的模型；三是周围的词使用同一个数学模型，但不是简单的等同处理，而是使用一个时间序列模型。

词本来是one hot encoding，通过矩阵变换就变成了word embedding，得到embedding之后(此处是word embedding)的处理，可以是concatenate，然后进一步做矩阵变换，或者得到的embedding就简单相加，得到句子的embedding.

20181124

word2vec是固定的词向量，不能解决多义词的问题。而ELMo (Embedding from Language Model)是考虑上下文的动态的词向量，可以在一定程度上解决多义词的问题。一般用双层双向lstm来做，任务可以是一般的分类任务，或者是像word2vec一样给句子的每个位置的词分类。最初的输入还是固定的词向量，第一层双向lstm，可以认为从上下文中抽取了句法信息，第二层双向lstm，可以认为从上下文中抽取了语义信息（但这种说法是不严谨的，并没有很强的证据来证明这样的说法）。不管怎么样，通过双层双向lstm，得到了包含上下文信息的word embedding.

word vector可以是静态的，也就是一个词只有一个意思，也可以是动态的，也就是在一个词的word embedding当中同时反映了上下文的信息，或者说，反映了这个词和其他词的相互作用，如果用的是cnn的方法，能反映远距离词的相互作用，但更多反映的是周围词的相互作用，如果用的是单向lstm，理论上能反映前面远距离词的相互作用，但实际上只能反映前面几个词与该词的相互作用，如果用的是self attention，就能反映与该词离得很远的词相互之间的作用。

在decoder当中，如果用的词向量是动态词向量（self attention或者与encoder中的词的attention-原来静态词向量作为查询，通过软寻址得到新的向量，体现该词和decoder或encoder中的其他词的相互作用；当然，对于lstm，用来表示句向量的hidden state也能作为查询，通过软寻址得到新的句向量，体现要预测的词和encoder的相互作用），那么条件语言模型中的条件就有一部分包含在了动态词向量当中，哪怕只是用传统的n-gram model来做语言模型（前n-1个词的动态向量组成的矩阵加上条件作为softmax的输入，预测的词作为输出，而且这个softmax的参数在预测不同的词时保持固定-变化的是输入，也就是马尔科夫链中的转移矩阵的分布保持不变），也能有很好的效果。此处的条件可以是固定的条件，比如一幅图片经过cnn扫描后得到的表示（如果条件做了attention，就是动态的条件）。

对语言模型（符合语法规范）来说，其实就两部分，一部分是之前的句子的表示，另一部分是一个softmax分类器，至于句子的表示，其实不只能用rnn，cnn和transformer也可以使用，只是一般的cnn的效果太差了。对于条件语言模型（除了符合语法规范外，还有具体的任务要完成），除了上面两部分，还多了两部分，一是条件怎么表示，二是条件的表示和之前的句子的表示怎么融和。

decoder：动态的单词(self attention)，动态的条件(attention)；静态的单词，动态的条件；动态的单词，静态的条件。

word2vec在做语言模型任务时，比如从中间的词预测周围的词（不是一般意义上的语言模型，因为语言模型预测词时很少只根据一个词来预测，除非是1-gram语言模型，一个词的预测只与上一个词是什么有关），用到中间的词的向量和周围词的向量，因为周围词是未知的，所以理论上所有词都要考虑，但因为是cross entropy，所以分子就只考虑文本中出现的词的词向量，而由于使用了negative sampling策略，分母中也只考虑文本中没出现过的少量词的词向量。

双层双向lstm得到了上下文的信息，但效果未必有简单的CBOW好，简单把上下文向量求平均也能得到关于上下文的信息。

word2vec和ELMo的拟合用的都是语言模型，也就是预测单词是什么，而且都使用的上下文的信息（有些语言模型只用上文的信息，不用下文的信息，比如输入法给提示的时候就用这样的语言模型）

GPT全称是generative(不是general?) pre-training，特征抽取用的是transformer，但只用了上文的信息，没用下文的信息。GPT还有个特点是多任务拟合，把多个任务使用相同的网络结构来拟合。这样，在下游任务中，transfer learing就不只是词向量了，而是整个网络。在多任务拟合上，salesforce的decanlp想法是类似的，只是网络结构不同。需要分析GPT、BERT、decanlp各自的网络结构，尤其是词向量动态表征之后的网络结构。

nlp的四大任务：序列标注，分类，句子关系判断（比如相似度判断，比如多轮对话中两句问话是否同一意图的关系判断，不如两句话是不是连着的两句话），生成式任务。

bert提出的masked语言模型是预测词的，同时还有预测句子的任务next sequence prediction，所以是多任务模型。预测词不是像word2vec一样每个词都预测，而是随机抠掉15%的词，用周围词的词向量来预测。

预测句子至少有两种办法，一种是语言模型，根据词的条件概率算出句子的概率，另一种是把句子当做整体，先算出句子的向量，然后像预测词一样利用句向量来预测句子。

在bert（的encoder）当中，每个单词有三个embedding，一是位置的embedding，二是单词的动态embedding，三是所在句子的embedding.但句子的embedding在bert当中怎么算，还需要具体看论文。位置的embedding，容易理解；单词的动态embedding，是该单词考虑和其他单词相互作用的结果；句子的embedding，是句子中的所有单词都考虑和其他单词相互作用的结果，然后取平均或者通过单层神经网络变换得到句向量。

多个向量转变为一个向量，一是靠直接相加求平均，一是靠concatenate然后用单层神经网络做变换。

两阶段模型就是先预训练词向量或者整个网络，之后两种方案，一种是冻住一部分网络，放开一部分网络（初始值取之前训练好的值），重新训练，还有一种是所有网络全部放开，但初始值取之前训练好的值。

一个类中不会改变对象属性的函数还是挺重要的，尤其是public的函数，最好是不会改变对象属性。否则的话，如果一个final类属性用该对象初始化，一旦该对象在之后执行这个函数，这个final类属性就变掉了（虽然引用不变），这样的话，在多线程中就是不安全的。

feedforward network是在同一层中的node相互之间没有相互作用，比如一般的fully connected network，而recurrent neural network在同一层中的node有方向性的作用，而这方向性的输出其实就是node的状态，也是rnn具有记忆功能的原因。

20181201

对于public的方法，最好不要改动对象的属性，package-private和protected的方法最好也不要。有一种用例是这样的，类的静态属性是final的，而且被赋值为某个对象。如果public方法是能改变对象的，那么，一旦该对象使用该方法，实际上这个对象就变掉了，就不是只读对象了，但final是允许的，final保证的只是引用始终指向这个对象。这样的话，在多线程中就是不安全的。

residual feedforward network其实就是一个前馈网络，g(f(x)+x)，通过这种变换，可以保证在做back propagation的时候，x本身（作为一个向量）的梯度变化不至于太小，因为通过前馈，把x往前提了一层。

attention在做Q对K的query的时候，Q和K的列一般是相同长度的向量（也可以不同，之后做变换就行），算attention weight的时候，一般是Q要做一个线性变换（引入更多自由度，向量长度可能变化，也可能不变），然后和K求关系值（比如相似度等，也可以是相关性，比如两个向量的person correlation coefficient）。拿到关系值以后，一般首先用softmax归一化（通过放在指数上，适当的扩大差别，更加突出重要元素的权重），用来给V加权，最后一般是加权后相加，也就是求加权平均，得到和Q（变换以后）长度相同的向量。通过求出attention weight，其实是做单词对齐的工作，看Q这个词和K中的那个词更有可能对齐。

神经网络是一种基本的运算模式，就好像加、减、乘、除一样，之前，在加减乘除和微积分的基础上，我们开发数学模型，计算在各个时间点上的输出，一般情况下，我们关心的是动态变化，而不是最后的均衡态（均衡态是一个理想模型，也就是参数的变化相比较状态的变化而言要小的多，而且要研究的状态总浓度是恒定的，在简单的物理系统中还有可能存在，但在复杂系统中，不仅状态在变，计算状态的各种主要参数也都在变，谈论均衡态没有意义）。在神经网路中，我们运用cnn、lstm、attention、feedforward、feedback等，用这些开发数学模型，计算在各个时间点上的输出。神经网络计算最大的问题有两个，一是算力不够，二是数据不够，这是不同于以往数学模型的。

对于图像，以往的数学模型也能有不错的建模，神经网络的建模是一种补充。但是，对于自然语言处理，以往的数学模型能力很差，神经网络的建模正在取代以往的数学模型。

一个时间序列，只有当最后输出的分布是一个固定分布的时候，才涉及到steady state，这在系统有外部物质和能量输入的情况下也是有可能的（比如，不断增加ATP，减少ADP来维持一个势能）。但是，如果输出不断在变（在语言模型中来说，每次预测下一个词时，softmax输出的概率分布不是一个恒定的分布），比如自然语言问题，就不涉及到steady state，因为不仅是开放系统，而且不断有新的信息输入。在生命系统中，其实steady state也没有太大的意义，真正有意义的是时间序列。

batch normalization: 对于sigmoid function或者tanh，batch normalization的意义在于，0周围函数的斜率较大，所以gradient较大，可以尽量避免gradient vanish。对于relu函数，batch normalization的意义在于，可以把数据调整到大约一半在0的左边，变成梯度为0的死节点，一半在0的右边，变成梯度可调节的点，关键在这个比例大概是一半一半。如果不做batch normalization，就要么很多死节点，要么梯度都在变，不能及时的淘汰某些节点，或者太多节点被淘汰。

统计模型强调随机性，比如广义线性模型，因为logistic regression可以由广义线性模型推导出来，所以，一定程度上logistic regression也是统计模型。神经网络模型的底层是统计模型，因为底层不管是线性回归还是逻辑回归，都是统计模型。但是，神经网络模型是一系列统计模型的组合，这种组合是有规律有机制的（虽然很难解释清楚，但是，有些物理模型也很难解释清楚啊，比如E=mc2，为什么这样组合，能说的清楚吗，但不代表没有机制），尤其是深度学习模型，组合是主要的，底层是次要的，底层是可换的（南大周志华教授不就把线性模型和sigmoid function换成了树模型来做底层吗？），所以，真正重要的是如何组合，底层用什么并不重要，现在既然神经网络可以用，那就用神经网络好了。

广义线性模型是在线性模型之后再加一个随机项，这样被预测的y也成了随机变量。真正的要预测的值是E(y)，也就是y的均值。当随机项的分布是高斯分布的时候，E(y)的表达式是线性回归，随机模式是高斯模式。当随机项的分布是伯努利分布（二项分布）时，E(y)的表达式是逻辑回归，随机模式是二项分布模式。当随机项的分布是多项式分布（多项分布）时，E(y)的表达式是softmax回归，随机模式是多项分布模式。

在构建loss function的时候，极大似然法与cross entropy有异曲同工之妙。对于极大似然法，只看数据中出现的标签所对应的logP(i)，对于cross entropy，看的是P_hat(i)logP(i)，但是，对于没出现的标签，P_hat(i)等于0，这样也就相当于极大似然法。对于logistic regression，不管是什么方法，都是当标签为1时，加的是logP，当标签为0时，加的是log(1-P)，综合起来就是P_hat(i)logP(i)+(1-P_hat(i)log(1-P(i))。

20181208

